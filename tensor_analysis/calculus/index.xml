<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Calculus :: 수리전자물리학</title>
    <link>http://localhost:1313/tensor_analysis/calculus/index.html</link>
    <description>(a) Gradient&#xD;(a) Divergence&#xD;(a) Curl</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="http://localhost:1313/tensor_analysis/calculus/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>(a) Gradient</title>
      <link>http://localhost:1313/tensor_analysis/calculus/a_gradient/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/tensor_analysis/calculus/a_gradient/index.html</guid>
      <description>(1) 데카르트 좌표계로 간주하는 매개변수 공간 또는 데카르트 좌표계 실공간에 대한 gradient 이다.&#xA;(2) Gradient 연산은 tensor의 차수를 하나 더 증가시킨다.&#xA;1. Scalar gradient $$&#xD;\nabla\varphi&#xD;=\frac{\partial \varphi}{\partial\vec{u}}&#xD;=\frac{\partial \varphi}{\partial u_i}\hat{u}_i&#xD;=\frac{\partial \varphi}{\partial u_1}\hat{u}_1+\frac{\partial \varphi}{\partial u_2}\hat{u}_+\frac{\partial \varphi}{\partial u_3}\hat{u}_3&#xD;$$$$&#xD;[\nabla\varphi]_i&#xD;=[\nabla]_i\varphi&#xD;=\frac{\partial \varphi}{\partial u_i}&#xD;$$ 2. Vector Gradient $$&#xD;\nabla\vec{f}&#xD;=\frac{\partial \vec{f}}{\partial\vec{u}}&#xD;=\frac{\partial f_j}{\partial u_i}\hat{u}_i\hat{u}_j&#xD;$$$$&#xD;[\nabla\vec{f}]_{ij}&#xD;=[\nabla]_i\vec{f}_j&#xD;=\frac{\partial f_j}{\partial u_i}&#xD;$$전개하면, 다음과 같으며, 2차 텐서임을 알 수 있다.&#xA;$$&#xD;\begin{align}&#xD;\nabla\vec{f}&#xD;= &amp; \frac{\partial f_1}{\partial u_1}\hat{u}_1\hat{u}_1 + \frac{\partial f_1}{\partial u_2}\hat{u}_2\hat{u}_1 + \frac{\partial f_1}{\partial u_3}\hat{u}_3\hat{u}_1 \\&#xD;&amp; \frac{\partial f_2}{\partial u_1}\hat{u}_1\hat{u}_2 + \frac{\partial f_2}{\partial u_2}\hat{u}_2\hat{u}_2 + \frac{\partial f_2}{\partial u_3}\hat{u}_3\hat{u}_2 \\&#xD;&amp; \frac{\partial f_3}{\partial u_1}\hat{u}_1\hat{u}_3 + \frac{\partial f_3}{\partial u_2}\hat{u}_2\hat{u}_3 + \frac{\partial f_3}{\partial u_3}\hat{u}_3\hat{u}_3&#xD;\end{align}&#xD;$$ 3. Conjugate vector gradient (잘못되었다고 생각 함) $$&#xD;\vec{f}\nabla&#xD;=\frac{\partial f_i}{\partial u_j}\hat{u}_i\hat{u}_j&#xD;$$$$&#xD;[\nabla\vec{f}]^{T}&#xD;=\vec{f}\nabla&#xD;$$ 4. Vector gradient &amp; Kronecker delta $$&#xD;\nabla\vec{u}&#xD;=\frac{\partial \vec{u}}{\partial \vec{u}}&#xD;=\frac{\partial u_i}{\partial u_j}\hat{u}_i\hat{u}_j&#xD;=[\delta_{ij}]&#xD;=\bar{\bar{I}}&#xD;$$$$&#xD;[\nabla\vec{u}]_{ij}&#xD;=\partial_i u_j&#xD;=\bar{\bar{I}}&#xD;$$ 5. High tensor gradient 2차 텐서에서,</description>
    </item>
    <item>
      <title>(a) Divergence</title>
      <link>http://localhost:1313/tensor_analysis/calculus/a_divergence/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/tensor_analysis/calculus/a_divergence/index.html</guid>
      <description>(1) 데카르트 좌표계로 간주하는 매개변수 공간 또는 데카르트 좌표계 실공간에 대한 gradient 이다.&#xA;(2) Divergence는 텐서의 차수를 하나 감소시킨다.&#xA;1. Divergence $$&#xD;\nabla\cdot\vec{f}&#xD;=\frac{\partial}{\partial\vec{u}}\cdot\vec{f}&#xD;=\frac{\partial}{\partial u_i}\hat{u}_i\cdot f_j\hat{u}_j&#xD;=\frac{\partial f_j}{\partial u_i}\hat{u}_i\cdot \hat{u}_j&#xD;=\frac{\partial f_j}{\partial u_j}&#xD;=\frac{\partial f_1}{\partial u_1}+\frac{\partial f_2}{\partial u_2}+\frac{\partial f_3}{\partial u_3}&#xD;$$$$&#xD;\nabla\cdot\vec{f}&#xD;=[\nabla]_j[\vec{f}]_j&#xD;=\frac{\partial f_j}{\partial u_j}&#xD;$$ example1) 벡터 $\vec{u}=u_1\hat{u}_1+u_2\hat{u}_2+u_3\hat{u}_3$ 에 대한 발산을 구하여라.&#xA;sol)&#xA;$$&#xD;\nabla\cdot\vec{u}&#xD;=\frac{\partial u_j}{\partial u_j}&#xD;=3&#xD;$$ 2. 2차 텐서의 divergence $$&#xD;\nabla\cdot\bar{\bar{A}}&#xD;=\frac{\partial}{\partial u_i}\hat{u}_i\cdot A_{jk}\hat{u}_j\hat{u}_k&#xD;=\frac{\partial A_{jk}}{\partial u_i}(\hat{u}_i\cdot\hat{u}_j)\hat{u}_k&#xD;=\frac{\partial A_{jk}}{\partial u_i}\delta_{ij}\hat{u}_k&#xD;=\frac{\partial A_{ik}}{\partial u_i}\hat{u}_k&#xD;$$$$&#xD;[\nabla\cdot\bar{\bar{A}}]_j&#xD;=[\nabla]_i\bar{\bar{A}}_{ij}&#xD;=\frac{\partial A_{ij}}{\partial u_i}&#xD;$$ 3. Laplacian operator $$&#xD;\nabla^2&#xD;=\nabla\cdot\nabla&#xD;=\frac{\partial}{\partial u_i}\hat{u}_i\cdot\frac{\partial}{\partial u_j}\hat{u}_j&#xD;=\frac{\partial^2}{\partial u_i \partial u_j}(\hat{u}_i\cdot\hat{u}_j)&#xD;=\frac{\partial^2}{\partial u_i \partial u_j}\delta_{ij}&#xD;=\frac{\partial^2}{\partial u_i \partial u_i}&#xD;=\frac{\partial^2}{\partial u_i^2}&#xD;$$$$&#xD;\nabla^2&#xD;=[\nabla]_i[\nabla]_i&#xD;=\frac{\partial^2}{\partial u_i^2}&#xD;$$</description>
    </item>
    <item>
      <title>(a) Curl</title>
      <link>http://localhost:1313/tensor_analysis/calculus/a_curl/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/tensor_analysis/calculus/a_curl/index.html</guid>
      <description>(1) 데카르트 좌표계로 간주하는 매개변수 공간 또는 데카르트 좌표계 실공간에 대한 gradient 이다.&#xA;(2) curl 은 텐서의 차수에 변화를&#xA;1. Curl $$&#xD;\nabla\times\vec{f}&#xD;=\frac{\partial}{\partial\vec{u}}\times\vec{f}&#xD;=\frac{\partial}{\partial u_i}\hat{u}_i\times f_j\hat{u}_j&#xD;=\frac{\partial f_j}{\partial u_i}\hat{u}_i\times\hat{u}_j&#xD;=\varepsilon_{ijk}\frac{\partial f_j}{\partial u_i}\hat{u}_k&#xD;$$$$&#xD;[\nabla\times\vec{f}]_k&#xD;=\varepsilon_{ijk}\partial_i f_j&#xD;$$ 2. Conjugate curl (잘못되었다고 생각 함) $$&#xD;\vec{f}\times\nabla&#xD;=f_i\hat{u}_i\times\frac{\partial}{\partial u_j}\hat{u}_j&#xD;=\frac{\partial f_i}{\partial u_j}\hat{u}_i\times\hat{u}_j&#xD;=\varepsilon_{ijk}\frac{\partial f_i}{\partial u_j}\hat{u}_k&#xD;$$ 3. High tensor curl 중복 지수 법칙에 위배하지 않아야 함을 주의한다.&#xA;$$&#xD;\nabla\times\bar{\bar{A}}&#xD;=\frac{\partial}{\partial u_i}\hat{u}_i\times A_{jk}\hat{u}_j\hat{u}_k&#xD;=\frac{\partial A_{jk}}{\partial u_i}\hat{u}_i\times\hat{u}_j\hat{u}_k&#xD;=\frac{\partial A_{jk}}{\partial u_i}(\hat{u}_i\times\hat{u}_j)\hat{u}_k&#xD;=\varepsilon_{ijl}\frac{\partial A_{jk}}{\partial u_i}\hat{u}_l\hat{u}_k&#xD;$$$$&#xD;[\nabla\times\bar{\bar{A}}]_{lk}&#xD;=\varepsilon_{ijl}\partial_i A_{jk}&#xD;$$ example1) $\nabla\times\nabla f=0$ 임을 보여라.</description>
    </item>
  </channel>
</rss>