<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mapping :: 수리전자물리학</title>
    <link>http://localhost:1313/tensor_analysis/mapping/index.html</link>
    <description>(a) Covariant and contravariant&#xD;(a) 2nd tensor&#xD;(a) Dot products&#xD;(a) Metric tensor</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="http://localhost:1313/tensor_analysis/mapping/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>(a) Covariant and contravariant</title>
      <link>http://localhost:1313/tensor_analysis/mapping/a_covariant_contravariant/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/tensor_analysis/mapping/a_covariant_contravariant/index.html</guid>
      <description>이전 챕터에서는 매개변수공간(데카르트 좌표계로 간주)에서 단위 기저 벡터에 대해서 다루었다. 그러나 모든 기저가 직교하지 않을 수도 있으며, 그 크기가 1이 아닐 수도 있다.&#xA;여기에서는 실공간의 일반 좌표계에서 기하학적 텐서를 표현하는 새로운 방식을 소개한다. orthogonal coordinates 에 대한 기본적인 이해가 있다면 학습에 도움이 될 것이다&#xA;1. 벡터의 표현 일반 좌표계에서 벡터의 “좌표값&#34;은 벡터를 해당 좌표축 벡터의 방향으로 평행하게 분해했을 때의 각 축에 대한 스케일링 계수 이다. suffix notation을 사용하여, 일반좌표계에서 벡터표현은 다음과 같다.</description>
    </item>
    <item>
      <title>(a) 2nd tensor</title>
      <link>http://localhost:1313/tensor_analysis/mapping/a_2nd-tensor/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/tensor_analysis/mapping/a_2nd-tensor/index.html</guid>
      <description>1. 2차 mixed tensor 표기법 2차 혼합 텐서의 경우, 지수 순서를 파악하기 위해 아래와 같이 표기한다.&#xA;$$&#xD;\bar{\bar{A}}&#xD;=A^{i}_{.j}\vec{g}_i\vec{g}^j&#xD;=A_{i.}^{\ j}\vec{g}^i\vec{g}_j&#xD;$$ 2. 2차 tensor의 전치 표기법 (1) sym 2차 tensor&#xA;$$&#xD;\bar{\bar{A}}^T&#xD;=(A^{ij})^T(\vec{g}_i\vec{g}_j)^T&#xD;=A^{ji}\vec{g}_j\vec{g}_i&#xD;=A^{ji}\vec{g}_i\vec{g}_j&#xD;$$$$&#xD;(A^{ij})^T=A^{ji},\quad&#xD;(A_{ij})^T=A_{ji},\quad&#xD;(A^i_{.j})^T=A_{j.}^{\ i},\quad&#xD;$$(2) skew 2차 tensor&#xA;$$&#xD;(A^{ij})^T=-A^{ji},\quad&#xD;(A_{ij})^T=-A_{ji},\quad&#xD;(A^i_{.j})^T=-A_{j.}^{\ i},\quad&#xD;$$ 3. 고차 tensor 표기법 $$&#xD;\stackrel{4}{A}&#xD;=A^{ij}_{..kl}\vec{g}^i\vec{g}^j\vec{g}_k\vec{g}_l&#xD;$$</description>
    </item>
    <item>
      <title>(a) Dot products</title>
      <link>http://localhost:1313/tensor_analysis/mapping/a_dot_products/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/tensor_analysis/mapping/a_dot_products/index.html</guid>
      <description>1. 고차 tensor 내적 When $\bar{\bar{\sigma}}=\sigma^{ij}\vec{g}_i\vec{g}_j$ and $\vec{n}=n^k\vec{g}_k$,&#xA;$$&#xD;\vec{T}&#xD;=\bar{\bar{\sigma}}\cdot\vec{n}&#xD;=\sigma^{ij}\vec{g}_i\vec{g}_j\cdot n^k\vec{g}_k&#xD;=\sigma^{ij}n^k\vec{g}_i\vec{g}_j\cdot \vec{g}_k&#xD;$$$$&#xD;=\sigma^{ij}n^k\vec{g}_ig_{jk}&#xD;$$$$&#xD;=\sigma^{ij}n_j\vec{g}_i&#xD;$$and also when $\bar{\bar{\varepsilon}}=\varepsilon_{kl}\vec{g}^k\vec{g}^l$&#xA;$$&#xD;W&#xD;=\frac12\bar{\bar{\sigma}}:\bar{\bar{\varepsilon}}&#xD;=\frac12\sigma^{ij}\vec{g}_i\vec{g}_j:\varepsilon_{kl}\vec{g}^k\vec{g}^l&#xD;=\frac12\sigma^{ij}\varepsilon_{kl}\vec{g}_i\vec{g}_j:\vec{g}^k\vec{g}^l&#xD;=\frac12\sigma^{ij}\varepsilon_{kl}(\vec{g}_i\cdot\vec{g}^k)(\vec{g}_j\cdot\vec{g}^l)&#xD;$$$$&#xD;=\frac12\sigma^{ij}\varepsilon_{kl}\delta_i^k\delta_j^l&#xD;$$$$&#xD;=\frac12\sigma^{ij}\varepsilon_{ij}&#xD;$$ 2. 대각합(trace) $$&#xD;\operatorname{tr}\bar{\bar{A}}&#xD;=\bar{\bar{A}}:\bar{\bar{I}}&#xD;=A^i_{.i}&#xD;$$proof)&#xA;$$&#xD;\operatorname{tr}\bar{\bar{A}}&#xD;=\bar{\bar{A}}:\bar{\bar{I}}&#xD;=A^{ij}\vec{g}_i\vec{g}_j:\vec{g}_k\vec{g}^k&#xD;=A^{ij}(\vec{g}_i\cdot\vec{g}_k)(\vec{g}_j\cdot\vec{g}^k)&#xD;$$$$&#xD;=A^{ij}g_{ik}\delta_j^k&#xD;=A^{ik}g_{ik}&#xD;=A^{i}_{.i}&#xD;$$ 4. 벡터의 크기 $$&#xD;||\vec{n}||&#xD;=\sqrt{n^in_i}&#xD;$$proof)&#xA;$$&#xD;||\vec{n}||&#xD;=\sqrt{\vec{n}\cdot\vec{n}}&#xD;=\sqrt{n^i\vec{g}_i\cdot n_j\vec{g}^j}&#xD;=\sqrt{n^in_j\vec{g}_i\cdot\vec{g}^j}&#xD;=\sqrt{n^in_j\delta_i^j}&#xD;=\sqrt{n^in_i}&#xD;$$ 5. 기저 벡터의 크기 $$&#xD;||\vec{g}_i||&#xD;=\sqrt{g_{\underline{i}\underline{i}}}&#xD;$$$$&#xD;||\vec{g}^i||&#xD;=\sqrt{g^{\underline{i}\underline{i}}}&#xD;$$여기에서 주의 해야할 점은, 지수 $i$ 는 중복 지수가 아니라는 것이다. 따라서, $\underline{i}$ 는 지수에 대한 중복법칙을 적용하지 말라는 의미이다.</description>
    </item>
    <item>
      <title>(a) Metric tensor</title>
      <link>http://localhost:1313/tensor_analysis/mapping/b_metric_tensor/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/tensor_analysis/mapping/b_metric_tensor/index.html</guid>
      <description>매트릭 텐서는 실공간[v]에서 두 기저 시스템, 즉 공변기저벡터($g_i$)와 반변기저벡터($g^i$) 사이의 관계를 정의하는 핵심적인 텐서이다. 이는 마치 두 언어 사이의 ‘사전’ 역할을 하며, 한 종류의 기저를 다른 종류의 기저로, 또는 한 종류의 성분을 다른 종류의 성분으로 변환하는 다리 역할을 한다.&#xA;1. 공변 기저 벡터의 매트릭 텐서 $$&#xD;g_{ij}=\vec{g}_i\cdot\vec{g}_j&#xD;$$매트릭 텐서를 행렬로 표시하면 다음과 같다.&#xA;$$&#xD;\left[g_{ij}\right]&#xD;=\begin{bmatrix}&#xD;g_{11} &amp; g_{12} &amp; g_{13} \\&#xD;g_{21} &amp; g_{22} &amp; g_{23} \\&#xD;g_{31} &amp; g_{32} &amp; g_{33} \&#xD;\end{bmatrix}&#xD;$$만약, 공변기저벡터들이 서로 ortgorhonal 하다면, 대각 성분을 제외한 모든 성분은 0이 된다. 공변기저벡터를 사용한 두 벡터의 내적은 매트릭 텐서를 사용하여 표현할 수 있다.</description>
    </item>
  </channel>
</rss>