<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Operation :: 수리전자물리학</title>
    <link>http://localhost:1313/tensor_analysis/operation/index.html</link>
    <description>(a) Dyad&#xD;(a) Dot products&#xD;(a) Unit tensor&#xD;(a) Seperation of tensor&#xD;(a) 4nd unit tensor&#xD;(a) Inverse &amp; Determinant&#xD;(a) Eigenvalue problem</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="http://localhost:1313/tensor_analysis/operation/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>(a) Dyad</title>
      <link>http://localhost:1313/tensor_analysis/operation/a_dyad/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/tensor_analysis/operation/a_dyad/index.html</guid>
      <description>1. Dyad 개념 Dyad란 두 벡터의 곱으로 이루어진 2차 tensor를 의미한다. 이때의 곱을 tensor 곱 또는 dyad 라고 한다. 표기법은 아래와 같다.&#xA;$$&#xD;\vec{a}\otimes\vec{b}&#xD;=\vec{a}\vec{b}&#xD;=\left[\begin{matrix}&#xD;a_1 \\ a_2 \\ a_3 \\ \vdots&#xD;\end{matrix}\right]&#xD;\left[\begin{matrix}&#xD;b_1 &amp; b_2 &amp; b_3 &amp; \cdots&#xD;\end{matrix}\right]&#xD;$$$$&#xD;[\vec{a}\vec{b}]_{ij}&#xD;=a_ib_j&#xD;$$ 2. 표기법 &amp; 교환법칙이 성립하지 않음 Dyad는 2차 tensor로서 성분과 기저 dyad로 구성된다.&#xA;$$&#xD;\vec{a}\otimes\vec{b}=a_{i}\hat{u}_{i}\otimes b_{j}\hat{u}_{j}=a_{i}b_{j}\left(\hat{u}_{i}\otimes\hat{u}_{j}\right)=\text{성분}\cdot\text{(기저 dyad)}&#xD;$$$$&#xD;[\vec{a}\vec{b}]_{ij}&#xD;=a_ib_j&#xD;$$일반적으로 dyad는 교환 법칙이 성립되지 않는다.</description>
    </item>
    <item>
      <title>(a) Dot products</title>
      <link>http://localhost:1313/tensor_analysis/operation/a_dot_products/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/tensor_analysis/operation/a_dot_products/index.html</guid>
      <description>1. Dyad와 벡터의 내적 $$&#xD;\left(\vec{a}\otimes\vec{b}\right)\cdot\vec{c}=\vec{a}\left(\vec{b}\cdot\vec{c}\right)&#xD;$$proof) 벡터 표기법&#xA;$$&#xD;\left(\vec{a}\otimes\vec{b}\right)\cdot\vec{c}=a_{i}\hat{u}_{i}\otimes b_{j}\hat{u}_{j}\cdot c_{k}\hat{u}_{k}=a_{i}b_{j}c_{k}\left(\hat{u}_{i}\otimes\hat{u}_{j}\cdot\hat{u}_{k}\right)&#xD;$$$$&#xD;=a_{i}b_{j}c_{k}\left(\hat{u}_{i}\delta_{jk}\right)&#xD;=a_{i}b_{j}c_{j}\hat{u}_{i}&#xD;=\vec{a}\left(\vec{b}\cdot\vec{c}\right)&#xD;$$proof) 성분 표기법&#xA;$$&#xD;[\vec{a}\vec{b}]_{ij}\vec{c}_j=a_{i}b_{j}c_{j}\implies&#xD;\vec{a}\left(\vec{b}\cdot\vec{c}\right)&#xD;$$ 2. Dyad와 dyad의 내적 Dyad와 dyad의 내적은 일반적인 행렬 곱을 의미한다. 이중 bar, $\bar{\bar{A}}$는 2차텐서를 의미한다.&#xA;$$&#xD;(1)&#xD;\bar{\bar{A}}\cdot\bar{\bar{B}}&#xD;=\left[\begin{matrix}&#xD;A_{11} &amp; A_{12} &amp; \cdots \\&#xD;A_{21} &amp; A_{22} &amp; \cdots \\&#xD;\vdots &amp; \vdots &amp; \ddots&#xD;\end{matrix}\right]&#xD;\left[\begin{matrix}&#xD;B_{11} &amp; B_{12} &amp; \cdots \\&#xD;B_{21} &amp; B_{22} &amp; \cdots \\&#xD;\vdots &amp; \vdots &amp; \ddots&#xD;\end{matrix}\right]&#xD;$$$$&#xD;(2)&#xD;\left(\vec{a}\otimes\vec{b}\right)\cdot\left(\vec{c}\otimes\vec{d}\right)=\left(\vec{b}\cdot\vec{c}\right)\vec{a}\vec{d}&#xD;$$proof2) 벡터 표기법</description>
    </item>
    <item>
      <title>(a) Unit tensor</title>
      <link>http://localhost:1313/tensor_analysis/operation/a_unit_tensor/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/tensor_analysis/operation/a_unit_tensor/index.html</guid>
      <description>1. 2차 단위 텐서 $$&#xD;\bar{\bar{I}}&#xD;=\left[\begin{matrix}&#xD;1 &amp; 0 &amp; 0 \\&#xD;0 &amp; 1 &amp; 0 \\&#xD;0 &amp; 0 &amp; 1&#xD;\end{matrix}\right]&#xD;=\left[\begin{matrix}&#xD;\delta_{11} &amp; \delta_{12} &amp; \delta_{13} \\&#xD;\delta_{21} &amp; \delta_{22} &amp; \delta_{23} \\&#xD;\delta_{31} &amp; \delta_{32} &amp; \delta_{33}&#xD;\end{matrix}\right]&#xD;=\delta_{ij}\hat{u}_i\hat{u}_j&#xD;=\hat{u}_j\hat{u}_j&#xD;$$$$&#xD;\bar{\bar{I}}\cdot\vec{a}&#xD;=\vec{a}&#xD;$$proof)&#xA;$$&#xD;\bar{\bar{I}}\cdot\vec{a}&#xD;=\delta_{ij}\hat{u}_i\hat{u}_j\cdot a_k\hat{u}_k&#xD;=\delta_{ij}a_k\hat{u}_i(\hat{u}_j\cdot\hat{u}_k)&#xD;=\delta_{ij}a_k\hat{u}_i\delta_{jk}&#xD;=a_j\hat{u}_j&#xD;=\vec{a}&#xD;$$ 2. 2차 단위 텐서 점곱 $$&#xD;\bar{\bar{I}}:\bar{\bar{I}}&#xD;=\hat{u}_i\hat{u}_i:\hat{u}_j\hat{u}_j&#xD;=(\hat{u}_i\cdot\hat{u}_j)(\hat{u}_i\hat{u}_j)&#xD;=\delta_{ij}\delta_{ij}&#xD;=\delta_{ii}&#xD;=3&#xD;$$ 3. 대각합 $$&#xD;\bar{\bar{A}}:\bar{\bar{I}}&#xD;=\operatorname{tr}\bar{\bar{A}}&#xD;=A_{jj}=A_{11}+A_{22}+A_{33}&#xD;$$대각합은 아래와 같은 성질을 가진다.</description>
    </item>
    <item>
      <title>(a) Seperation of tensor</title>
      <link>http://localhost:1313/tensor_analysis/operation/a_seperation_of_tensor/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/tensor_analysis/operation/a_seperation_of_tensor/index.html</guid>
      <description>1. 대칭 및 비대칭 tensor 이중 bar $\bar{\bar{A}}$는 2차텐서를 의미한다.&#xA;(1) 대칭 텐서&#xA;아래를 만족할 때, 대칭텐서라고 한다.&#xA;$$&#xD;\bar{\bar{A}}=\bar{\bar{A}}^T,\quad&#xD;A_{ij}=A_{ji}&#xD;$$(2) 비대칭 텐서&#xA;아래를 만족할 때, 비대칭 텐서라고 한다.&#xA;$$&#xD;\bar{\bar{A}}=-\bar{\bar{A}}^T,\quad&#xD;A_{ij}=-A_{ji}&#xD;$$(3) 대칭 텐서와 비대칭 텐서의 분리&#xA;대칭 비대칭의 성질을 이용하여, 임의이 텐서를 아래와 같이 쓸 수 있다.&#xA;$$&#xD;\bar{\bar{A}}=\operatorname{sym}\bar{\bar{A}}+\operatorname{skew}\bar{\bar{A}}&#xD;$$여기에서, sym과 skew는 대칭과 비대칭을 의미하며, 다음과 같이 정의된다.&#xA;$$&#xD;\operatorname{sym}\bar{\bar{A}}&#xD;=\frac{1}{2}\left(\bar{\bar{A}}+\bar{\bar{A}}^T\right)&#xD;$$$$&#xD;\operatorname{skew}\bar{\bar{A}}&#xD;=\frac{1}{2}\left(\bar{\bar{A}}-\bar{\bar{A}}^T\right)&#xD;$$proof)&#xA;$$&#xD;\bar{\bar{A}}=\operatorname{sym}\bar{\bar{A}}+\operatorname{skew}\bar{\bar{A}}&#xD;$$$$&#xD;\bar{\bar{A}}^T=\operatorname{sym}\bar{\bar{A}}^T-\operatorname{skew}\bar{\bar{A}}^T&#xD;$$따라서,</description>
    </item>
    <item>
      <title>(a) 4nd unit tensor</title>
      <link>http://localhost:1313/tensor_analysis/operation/a_4nd_unit_tensor/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/tensor_analysis/operation/a_4nd_unit_tensor/index.html</guid>
      <description>1. 4차 단위 텐서 $$&#xD;\stackrel{4}{I}=\delta_{ik}\delta_{jl}\hat{u}_i\hat{u}_j\hat{u}_k\hat{u}_l&#xD;$$$$&#xD;\bar{\bar{A}}=\stackrel{4}{I}:\bar{\bar{A}}&#xD;$$각 성분과 값을 나열해 보자. $(i,j,k,l;\text{값})$&#xA;$$&#xD;(1,1,1,1;1), (2,2,2,2;1), (3,3,3,3;1)&#xD;$$$$&#xD;(1,1,1,2;0), (1,1,1,3;0), (1,1,1,4;0)&#xD;$$$$&#xD;(1,2,1,2;1), (1,3,1,3;1), (1,4,1,4;1) $$$$&#xD;(1,1,2,2;0), (1,1,3,3;0), (1,1,4,4;0) $$$$&#xD;(1,2,2,1;0), (1,3,3,1;0), (2,1,1,2;0) $$proof)&#xA;$$&#xD;\bar{\bar{A}}=\stackrel{4}{I}:\bar{\bar{A}}&#xD;$$$$&#xD;A_{mn}\hat{u}_m\hat{u}_n&#xD;=C_{ijkl}\hat{u}_i\hat{u}_j\hat{u}_k\hat{u}_l:A_{mn}\hat{u}_m\hat{u}_n&#xD;$$아래와 같이 정리할 수 있다.&#xA;$$&#xD;\hat{u}_m\hat{u}_n&#xD;=C_{ijkl}\hat{u}_i\hat{u}_j\hat{u}_k\hat{u}_l:\hat{u}_m\hat{u}_n&#xD;=C_{ijkl}\hat{u}_i\hat{u}_j(\hat{u}_k\hat{u}_l:\hat{u}_m\hat{u}_n)&#xD;=C_{ijkl}\hat{u}_i\hat{u}_j(\hat{u}_k\cdot\hat{u}_m)(\hat{u}_l\cdot\hat{u}_n)&#xD;$$$$&#xD;=C_{ijkl}\hat{u}_i\hat{u}_j\delta_{km}\delta_{ln}&#xD;=C_{ijmn}\hat{u}_i\hat{u}_j&#xD;$$따라서,&#xA;$$&#xD;C_{ijmn}&#xD;=\delta_{im}\delta_{jn}&#xD;$$m를 k로 바꾸고, n을 l로 바꾸면,&#xA;$$&#xD;C_{ijkl}&#xD;=\delta_{ik}\delta_{jl}&#xD;$$ 2. 전치 변환 (4차)텐서 $\stackrel{4T}{I}$ 텐서는 ‘4차 단위 텐서’의 전치가 아님을 주의한다. 2차 텐서가 주어졌을 때, 이 2차 텐서를 전치 텐서로 변환시키는 연산자이다.</description>
    </item>
    <item>
      <title>(a) Inverse &amp; Determinant</title>
      <link>http://localhost:1313/tensor_analysis/operation/a_inverse_determinant/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/tensor_analysis/operation/a_inverse_determinant/index.html</guid>
      <description>Tensor 방정식을 사용하여 연산을 하는 많은 경우에 tensor의 역을 구해야 할 필요가 있다.&#xA;역행렬을 손으로 계산하기에 적당한 크기는 3X3의 행렬식으로 표시가 가능한 2차 tensor 정도이며, 그 이상은 계산량의 증가로 인하여 해석적인 방법은 실제 사용하지 않는다.&#xA;1. Tensor의 역(inverse) $$&#xD;\bar{\bar{A}}\cdot\bar{\bar{A}}^{-1}&#xD;=\bar{\bar{I}}&#xD;$$$\bar{\bar{A}}^{-1}$의 지수표현은&#xA;$$&#xD;\left[A_{ij}\right]^{-1}&#xD;$$ 2. 행렬식(determinant) $$&#xD;\operatorname{det}\bar{\bar{A}}&#xD;=\operatorname{det}\left[\begin{matrix}&#xD;A_{11} &amp; A_{12} &amp; A_{13} \\&#xD;A_{21} &amp; A_{22} &amp; A_{23} \\&#xD;A_{31} &amp; A_{32} &amp; A_{33}&#xD;\end{matrix}\right]&#xD;$$$$&#xD;=A_{11}\hat{A}_{11}+A_{11}\hat{A}_{12}+A_{11}\hat{A}_{13}&#xD;$$$$&#xD;=A_{21}\hat{A}_{21}+A_{22}\hat{A}_{22}+A_{23}\hat{A}_{23}&#xD;$$$$&#xD;=A_{31}\hat{A}_{31}+A_{32}\hat{A}_{32}+A_{33}\hat{A}_{33}&#xD;$$여기에서,</description>
    </item>
    <item>
      <title>(a) Eigenvalue problem</title>
      <link>http://localhost:1313/tensor_analysis/operation/a_eigenvalue_problem/index.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/tensor_analysis/operation/a_eigenvalue_problem/index.html</guid>
      <description>공사중</description>
    </item>
  </channel>
</rss>